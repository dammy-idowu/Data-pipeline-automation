# ğŸš€ Professional Data Pipeline & Real-Time Analytics Dashboard
A production-grade ETL (Extract, Transform, Load) pipeline built with Python, SQL, and Aiven Cloud PostgreSQL. This project automates the extraction of "dirty" enterprise data, performs complex cleaning (deduplication, standardization, imputation), and monitors business health through a real-time visualization dashboard with automated alerting.

# ğŸ—ï¸ Architecture & Features
Cloud Infrastructure: Hosted on Aiven PostgreSQL for high availability and secure SSL connections.
Automated ETL: A modular DataPipeline class using SQLAlchemy to handle complex joins and data cleaning logic.
Intelligent Transformation:
Deduplication: Merges redundant customer and transaction records.
Standardization: Fixes casing errors (e.g., BOB SMITH â†’ Bob Smith) and trims whitespace.
Imputation: Handles missing emails and orphan support tickets using professional business logic.
Real-Time Monitoring: A Seaborn & Matplotlib dashboard refreshing every 30 seconds.
Job Scheduling: Background task orchestration via APScheduler running every 10 minutes.
Reliability & Alerting: An SMTP Alerting System that triggers email notifications on database connection failures or pipeline crashes.

ğŸ› ï¸ Technology Stack
Category	Tools
Language	Python 3.10+
Database	PostgreSQL (Hosted on Aiven Cloud)
Orchestration	APScheduler
Data Processing	Pandas, NumPy
Database Toolkit	SQLAlchemy, Psycopg2
Visualization	Matplotlib, Seaborn
Security	python-dotenv (Environment Variables), SSL/TLS Encryption

ğŸ“‚ Project Structure
text
â”œâ”€â”€ .env                # Secret credentials (Database URI, SMTP details)
â”œâ”€â”€ .gitignore          # Excludes .env and caches from version control
â”œâ”€â”€ main.ipynb          # The Jupyter Notebook containing the end-to-end pipeline
â”œâ”€â”€ requirements.txt    # List of required Python libraries
â””â”€â”€ README.md           # Professional project documentation
Use code with caution.

âš™ï¸ Setup & Installation
1. Database Setup
Create a free PostgreSQL instance on Aiven.io.
Obtain your Service URI.
2. Environment Configuration
Create a .env file in the root directory:
text
AIVEN_SERVICE_URI=postgresql+psycopg2://avnadmin:password@host:port/defaultdb?sslmode=require
EMAIL_USER=your-email@gmail.com
EMAIL_PASS=your-app-specific-password
Use code with caution.

3. Installation
bash
pip install -r requirements.txt
Use code with caution.

ğŸ“ˆ Pipeline Execution Logic
Seed: The script generates 20 rows of "Dirty Data" (duplicates, nulls, casing errors) and pushes them to the cloud.
Extract: The pipeline executes an optimized SQL query against the raw_enterprise_data table.
Transform:
Cleans customer_name using .str.title().str.strip().
Imputes missing ticket_id and status.
Flags negative amount values as is_refund.
Visualize: The dashboard updates every 30 seconds to show the latest Revenue Trends and Support Ticket Volume.
Monitor: If the Aiven connection drops, the send_alert function dispatches an emergency email.

ğŸ›¡ï¸ Security Best Practices
Credential Masking: No passwords are hardcoded; all secrets are managed via os.getenv.
Database Safety: Connection pooling is implemented with pool_pre_ping=True to handle serverless timeouts gracefully.
Error Handling: The system uses try-except blocks to "fail loudly" via logs and emails without crashing the main application.

ğŸ‘¨â€ğŸ’» Author
Damilola Idowu
Full stack Data Analyst / Data Scientist
[Your LinkedIn Profile Link] | [Your Portfolio Link]
